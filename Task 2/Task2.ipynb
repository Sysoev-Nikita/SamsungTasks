{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Никита Сысоев\n",
    "# Задание 2\n",
    "\n",
    "### 2.1\n",
    "Для решения задачи классификации я применил полносвязную нейронную сеть и модель работы с текстом **\"Bag of words\"**. \n",
    "\n",
    "Считываются тексты книг из папки для обучения. Эти тексты разбиваются на предложения, и для каждого предложения отмечается автор. Каждое предложение приводится в \"нормальный\" вид: все слова ставятся в нормальную форму, удаляются знаки препинания и числа.\n",
    "\n",
    "Заполняется словарь частот слов из обучающих предложений.\n",
    "Каждое предложение преврящается в массив, в котором для каждого слова из словаря указана частота его употребления в данном предложении.\n",
    "\n",
    "Полученные массивы подаются на вход полносвязной нейронной сети с одним скрытым слоем. Сеть обучается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Embedding\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RND_SEED = 7\n",
    "\n",
    "np.random.seed(RND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_fb2(path):\n",
    "    \"\"\"Returns parsed fb2 book as string\"\"\"\n",
    "    text_file = open(path, encoding='utf8')\n",
    "    text = text_file.read()\n",
    "    \n",
    "    #Remove marks\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    text = re.sub(r'[\\n\\t]', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scan_fb2_books(path):\n",
    "    \"\"\"Returns tuples of book's name and text for each fb2 book in directory\"\"\"\n",
    "    books = []\n",
    "    for f in os.listdir(path):\n",
    "        if (f.endswith('.fb2')):\n",
    "            books.append((f,get_text_fb2(path + f)))\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    \"\"\"Returns all sentences in text\"\"\"\n",
    "    sentences = re.split(r' *[\\.\\?!…][\\'\"\\)\\]]* *', text)\n",
    "    sentences = filter(lambda s: len(s) > 12, sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text, cache={}, morphy_analyzer=MorphAnalyzer()):\n",
    "    \"\"\"Normalizes text.\n",
    "    \n",
    "    Puts each word in it's normal form, removes all numbers, punctuation marks and words shorter than 4.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to normalize\n",
    "        cache (dict): Dictionary of already known normal forms (speeds up the calculation)\n",
    "        morphy_analyzer (MorphAnalyzer): Analyzer used for putting a word in in't normal form\n",
    "    \"\"\"\n",
    "    text_without_numbers = re.sub(r'[0-9]+', '', text)\n",
    "    words = text_to_word_sequence(text_without_numbers)\n",
    "    text_norm = ''\n",
    "    for word in words:\n",
    "        w_low = word.lower()\n",
    "        if w_low not in cache:\n",
    "            #Put new word normal form in cache\n",
    "            cache[w_low] = morphy_analyzer.parse(w_low)[0].normal_form\n",
    "        if (len(cache[w_low]) > 3):\n",
    "            text_norm += \" \" + cache[w_low]\n",
    "\n",
    "    return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hegel_path = \"./Data/Hegel/\"\n",
    "gogol_path = \"./Data/Gogol/\"\n",
    "\n",
    "he_books = scan_fb2_books(hegel_path)\n",
    "go_books = scan_fb2_books(gogol_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All sentences of Hehel and Gogol\n",
    "he_sents = [sent for book in he_books for sent in get_sentences(book[1])]\n",
    "go_sents = [sent for book in go_books for sent in get_sentences(book[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 20000 #Number of sentences for training and testing\n",
    "#Equal number of sentences by Hegel and Gogol\n",
    "he_sents_sample = he_sents[:sample_size // 2]\n",
    "go_sents_sample = go_sents[:sample_size // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {} #Cache for words normal form\n",
    "morph_an = MorphAnalyzer() \n",
    "#Normalized sentences of Hegel and Gogol\n",
    "he_sents_norm = [normalize(sent, cache, morph_an) for sent in he_sents_sample]\n",
    "go_sents_norm = [normalize(sent, cache, morph_an) for sent in go_sents_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = he_sents_norm + go_sents_norm\n",
    "#List of tuples (H,G), where H and G are probabilities of a text to be Hegel's and Gogol's\n",
    "authors = []  \n",
    "for sent in he_sents_norm:\n",
    "    authors.append((1,0))\n",
    "for sent in go_sents_norm:\n",
    "    authors.append((0,1))\n",
    "sentences = np.array(sentences)\n",
    "authors = np.array(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, test_sents, y_train, y_test = train_test_split(sentences, authors, train_size=0.8, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of words in tokenizer's vocabulary\n",
    "vocab_size = 1000\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using tokenizer to calculate the frequency of words in sentences for each word in tokenizer's vocabulary\n",
    "X_train = tokenizer.texts_to_matrix(train_sents, mode='freq')\n",
    "X_test = tokenizer.texts_to_matrix(test_sents, mode='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape=(vocab_size,), \n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 15s 958us/step - loss: 0.2838 - acc: 0.9005 - val_loss: 0.2060 - val_acc: 0.9223\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.1671 - acc: 0.9353 - val_loss: 0.1940 - val_acc: 0.9287\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.1451 - acc: 0.9432 - val_loss: 0.1923 - val_acc: 0.9305\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 6s 373us/step - loss: 0.1328 - acc: 0.9476 - val_loss: 0.1942 - val_acc: 0.9310\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 8s 469us/step - loss: 0.1220 - acc: 0.9517 - val_loss: 0.1950 - val_acc: 0.9317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e7f3acca90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=16, verbose=1, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве меры качества использовалась **accuracy**, на тестовой выборке **accuracy=0.93**. \n",
    "\n",
    "Интерпретировать это можно как то, что модель работает хорошо, однако точная цифра мало что дает, поскольку качество зависит от размера текста - чем больше текст, тем выше качество. Поэтому 0.93 на коротких предложениях - это хороший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Для вывода предложений, похожих и на Гоголя, и на Гегеля, предскажем вероятность для обоих классов с помощью нашей модели и выведем 30 предложений с наибольшим произведением вероятностей (таким образом в топе оказываются предложения с большими вероятностями обоих классов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting word frequency array for all sentences\n",
    "sentences_mtx = tokenizer.texts_to_matrix(sentences, mode='freq')\n",
    "pred = model.predict(sentences_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Original sentences (unnormalized)\n",
    "sentences_orig = he_sents_sample + go_sents_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting sentences by multiplication of probabilities (moving (1,1) to the top and (0,0) to the bottom)\n",
    "sorted_by_uncert = [s for s,_ in sorted(zip(sentences_orig, pred), key=lambda pair: pair[1][1] * pair[1][0], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "«В Афинах, как рассказывают, все выбежали из мастерских, чтобы посмотреть на него, и, когда ему кто-то сказал, что все смотрят на него с удивлением, как на невиданного зверя, он ответил: «нет, как на настоящего человека»[73]\n",
      "Этот вопрос все снова и снова повторяется, снова и снова прибавляется одно зернышко или вырывается один волос\n",
      "Книгу, в которой ничего не написано, каждый может понять\n",
      "Если, например, исчезает человек, то нет ни рук, ни ног помимо названия, как, например, мы называем рукой каменную руку, ибо отрезанная рука все равно, что каменная»\n",
      "Нужно вообще сказать, что здесь начинает все более и более бросаться в глаза рядоположность\n",
      "Ибо по какому другому соображению никто из них не сделал землю элементом, как это делает народное представление\n",
      "Подробности излагаются источниками различно и согласно некоторым из них, ему, между прочим, ставилась в вину хвалебная песнь Гермию и надпись на посвященной последнему статуе\n",
      "Соблазнил ли он хоть кого-нибудь – и в особенности тех, с которыми он общался – примером, который он им давал\n",
      "последний шарик отскакивает и попадает в душу\n",
      "Натешься же, козацкая душа, в последний раз\n",
      "297, 33), поскольку он подтверждается началом разговора\n",
      "«Существуют венцы, спле{225}тенные друг над другом; один из них всегда состоит из неплотного, а другой из плотного, а между ними находятся другие, смешанные из света и тьмы; более узкие венцы состоят из нечистого огня, а те, которые над ними, – из ночи, через которую проходит сила огня\n",
      "274         Здесь и несколькими строками ниже «положительная философия» употребляется в совершенно другом значении, чем то, в котором оно, как мы только что видели, дважды употреблялось Гегелем, так как там умозрение противополагается догматизму\n",
      "На той же основе возникли и другие образы поэмы: и оскотинившийся Собакевич, и страшный Плюшкин — „прореха на человечестве“, пустивший по миру сотни живых душ; и усложненный вариант Хлестакова — Ноздрев, и Чичиков — „любезнейший и обходительнейший человек“ — идеал приспособленчества и темный делец уже новой формации\n",
      "{133}               ЧАСТЬ ПЕРВАЯ\n",
      "Обо всем этом мы должны еще сказать подробнее\n",
      "Должно быть, больно поколотил вражий сын\n",
      "„Такие ли должны быть индейки\n",
      "Задают относительно нее вопрос, «знала ли она стоявшего перед нею брата Ореста, или нет\n",
      "Я вас уверяю, что жиру в одной больше, чем в десятке таких, как эти\n",
      "Ищи себе какого хочешь жениха, дурачь кого хочешь; а меня не увидишь уже больше на этом свете“\n",
      "Впечатление от его поэмы слагается из совокупности всех составляющих ее элементов — от реалистических разоблачений до страниц лирических раздумий\n",
      "Один представлял какого-то архиерея, другой Петра III\n",
      "Рассматривая, он заметил на другой стороне ее группу из двух-трех человек, лежавших почти без всякого движения на земле\n",
      "Позднейший автор, Прокл (in Timaeum, p\n",
      "разделены в возможности, они должны быть разделены также и в действительности, ибо в противном {237}случае их нельзя было бы делить до бесконечности\n",
      "Я мудрее некоторых и смелее других; я мудрее таких, которые не замечают обмана Писистрата, и смелее тех, которые видят этот обман, но из страха молчат»\n",
      "Мне легче два раза в год съездить в Миргород, в котором, вот уже пять лет, как не видал меня ни подсудок из земского суда, ни почтенный иерей, чем показаться в этот великой свет\n",
      "иначе мы бы видели его на другой стороне\n",
      "Чувства, страсти обитают именно в груди, в сердце (в сердце мы помещаем бессмертное), духовное же находится в голове\n"
     ]
    }
   ],
   "source": [
    "show_uncert = 30\n",
    "for s in range(show_uncert):\n",
    "    print(sorted_by_uncert[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что сюда попало несколько неправильно разбитых предложений, а также предложения, в которых речь идет о довольно абстрактных вещах, упоминаемых обоими авторами (душа, любовь, грамота т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "Проверим нашу модель на книгах Гоголя и Гегеля, не участвовавших в обучении и тестировании"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, tokenizer, model):\n",
    "    \"\"\"Predicts probability for text to be Hegel's or Gogol's\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text for classification\n",
    "        tokenizer (Tokenizer): Tokenizer with filled vocabulary\n",
    "        model (Sequential): Model used to make prediction\n",
    "    \"\"\"\n",
    "    text_norm = normalize(text)\n",
    "    freqs = tokenizer.texts_to_matrix((text_norm,), mode='freq')\n",
    "    return model.predict(freqs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Books not involved in training\n",
    "test_he_path = \"./Test/hegel/\"\n",
    "test_go_path = \"./Test/Gogol/\"\n",
    "\n",
    "test_he_books = scan_fb2_books(test_he_path)\n",
    "test_go_books = scan_fb2_books(test_go_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hegel:\n",
      "Лекции по истории философии. Книга третья.fb2\n",
      "[0.9967961  0.00320395]\n",
      "Философия истории.fb2\n",
      "[0.9930033  0.00699667]\n",
      "Философия права.fb2\n",
      "[0.99589986 0.00410013]\n",
      "Gogol:\n",
      "Том 4. Ревизор.fb2\n",
      "[0.0118834 0.9881166]\n",
      "Том 5. Женитьба. Драматические отчерки.fb2\n",
      "[0.01230689 0.98769313]\n",
      "Том 6. Мертвые души. Том 1.fb2\n",
      "[0.01414921 0.98585075]\n",
      "Том 7. Мертвые души. Том 2.fb2\n",
      "[0.02441487 0.97558516]\n"
     ]
    }
   ],
   "source": [
    "print(\"Hegel:\")\n",
    "for book in test_he_books:\n",
    "    print(book[0])\n",
    "    print(predict(book[1], tokenizer, model))\n",
    "\n",
    "print(\"Gogol:\")\n",
    "for book in test_go_books:\n",
    "    print(book[0])\n",
    "    print(predict(book[1], tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно видеть, большие тексты модель классифицирует вполне уверенно"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
